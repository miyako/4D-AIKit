# AIKit

## Overview

[4D AIKit](https://github.com/4d/4D-AIKit) is a built-in 4D component that enables interaction with third-party AI APIs.

This repo was forked for experimentation and testing purposes.

Normally you should use the offical releases.

## Install

Add `https://github.com/miyako/AIKit/` (without the official `4D-` prefix) to `dependencies.json`

## Abstract: Structured Outputs and Tool Calling

4D 21 added [**tool calling**](https://blog.4d.com/4d-21-and-ai-kit-redefining-how-applications-think-and-act/) to AI Kit. It allows the AI to make queries to your database. The feature is especially useful with thinking models or reasoning models that can plan a sequence of function calls to complete a multi-step task.

Another important addition in 4D 21 is [**structured outputs**](https://blog.4d.com/4d-aikit-structured-outputs/). It allows the AI to exchange information in a format that other computer programs can understand. Without structured outputs the AI can only interface with people (or other AI, I guess...), not machines. 

Tool calling and structured outputs depend not just on the models themselves but on the backend inference engine. To give an example, The **Kimi K2 Thinking** model performs well on its native platform but poorly on other platforms. Same for **Phi**, **Qwen**, **DeepSeek**, or **Gemma** on **Azure OpenAI**. For tool calling and structured outputs you need the right combination of prompt, model, and engine.

## Instruct vs. Reasoning vs. Thinking

LLMs have gone though several phases of evolution in the past couple of years. The most notable of which is that there are now several kinds of models.

An **instruct** model is trained, or fine-tuned, to follow instructions than simply engage in a casual conversaion. It uses its LLM training to dtermine what the users wants and responds accrodingly. It will only use tools when specifcally asked to. It is the right kind of AI to use as an interface for simple automation tasks. On the other hand, an instuct model may fail if the prompt is too abstract or lacks strategy.

A **reasoning** model is specifically trained in logic. Unlike earlier models that are trained to give the most likely response, the model reflects on its generated output and tries to make sense of it. while it may sound more intelligent, it has the potential to disobey instructions that seem, well, unreasonable. For simple automation tasks, the reasoning may actually backfire and result in hallucination or a chain of thought stuck in a rut.

A **thinking** model is trained to think which course of action to take in order to satisfy the user's demand. The thinking process consumes a massive amount of tokens and increases the time the model takes to make the first move. It also introduces a layer of unpredictability as to what the model will actually do to reach its goal. You do not need a thinking model if the prompt is already well thought out. 

## Hyper Parameters

Hyper parameters are levers that control the model's temperament. Finding the perfect combination is critical for tasks that need to balance creative thinking and rules following. This is especially true for smaller models that have limited intelligence.

|Parameter|Description|
|-|-|
|`temperature`|Increase to encourage wild thinkning. Decrease to avoid syntax drift. `0` recommended for tool calling.|
|`top-p`|Increase to include the less likely "long tail" tokens. Decrease to cut such tokens off. `0.9` recommended for structured output.|
|`min-p`|Increase to include exotic tokens. Decrease to cut such tokens off. Preferred over top-k. `0.05` recommended for structured output.|
|`top-k`|Limits how many possible tokens to consider. `1` recommended for structured output.|
|`repeat-penalty`|Increase to penalise repeating tokens. Decrease to allow such tokens. `40` is generally considered a safe value.|

## Code Cutter

I tested all major open weight models, with 4D 21 and AI Kit, to see how well they support tool calling and structured output. I used the same demo that was used in the 21 beta webinar.

In each table, you will see 2 icons 

âœ…<br/>âŒ

The one on top refers to structured output, whether the model output successfully validated against the JSON schema. 

The one under it refers to tool calling, whether the model made requests to use tools in the right format and whether t used the results given in response. 

> [!TIP]
> Tool calling is a form of chat completion. In a standard chat, each party plays the role of a "user" and an "assistant", taking turns in an ongoing conversation. When tool calling is enabled, the AI may send a structured function call, to which the other side is expected to respond in a "tool" role instead of a "user" role.
 
## Flagship Open Weight Models

> [!WARNING]
> These models are technically open weight but too large to run on any consumer equipment. You need a cloud provider.

|Model|FireWorks AI|Azure OpenAI|Moonshot AI|DeepInfra
|-|:-:|:-:|:-:|:-:|
|Kimi K2 Thinking|âœ…<br/>âŒ|âŒ<br/>âŒ|âœ…<br/>âœ…|âœ…<br/>âŒ
|DeepSeek R1 0528|âŒ<br/>âŒ|âŒ<br/>âŒ||âœ…<br/>âŒ
|Cogito 671B v2|âŒ<br/>âŒ|

### Llama

**Llama**  is an open weight model developed by Meta. Many versions of the model are hosted on different platforms.

Although LLama 3.1 and beyond technically supports structured output and tool calling, a small (`1` or `3` billion parameters) quantised model is too dumb to output reliable results.

|Model|llama.cpp|Azure OpenAI|
|-|:-:|:-:|
|Llama 3.1 8B Instruct| âŒ<br/>âŒ
|Llama 3.2 1B Instruct|âŒ<br/>âŒ
|Llama 3.2 3B Instruct|âŒ<br/>âŒ
|Llama 3.3 70B Instruct| |âŒ<br/>âŒ||
|Llama 3.1 405B| |âŒ<br/>âŒ||
|Llama 4 Scout 17B Instruct| |âŒ<br/>âŒ||
|Llama 4 Maverick 17B Instruct| |âŒ<br/>âŒ||

### Phi 

**Phi** is an open weight model developed by Microsoft. Several variants of the model are hosted on [Microsoft Foundry](https://azure.microsoft.com/en-us). 

[**Phi 4 Mini Instruct**](https://huggingface.co/microsoft/Phi-4-mini-instruct) and [Phi 4 Multimodal Instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) supports function calls but not [Phi 4 Mini Reasoning](https://huggingface.co/microsoft/Phi-4-mini-reasoning) or the original [Phi 4](https://huggingface.co/microsoft/phi-4). Because the reasoning models do not support function calls, you need to give clear instructions on how to use the tools and what to do with the results; you can't expect the model to plan a sequence of function calls. If you give a reasoning task to the instruct model, it will keep making the same function calls and eventually run out of tokens:

> the request exceeds the available context size, try increasing it

Phi 4 supports structured output on llama.cpp but not on Microsoft Foundry. The backend server framework used by Microsoft Azure OpenAI evidently implements the older "JSON Mode" standard not the newer "Structured Outputs" standard. 

|Model|llama.cpp|Azure OpenAI
|-|:-:|:-:|
|Phi 4 Mini Flash Reasoning|&nbsp;<br>&nbsp;||
|Phi 4 Mini Reasoning Plus|âœ…<br/>âŒ|
|Phi 4 Mini Reasoning|âœ…<br/>âŒ|âŒ<br/>âŒ|
|Phi 4 Mini Instruct|âœ…<br/>âš ï¸|âŒ<br/>âŒ
|Phi 4 Reasoning Plus|âœ…<br/>âŒ
|Phi 4 Reasoning|âœ…<br/>âŒ|âŒ<br/>âŒ||
|Phi 4 |âœ…<br/>âŒ|âŒ<br/>âŒ
|Phi 4 Multimodal Instruct||âŒ<br/>âŒ

### Gemma

**Gemma** is an open weight model developed by Google. There are pre-trained (Pt) and instruction tuned (It) models. The "N" variants are specifically designed to run better on mobile devices. 

Function calls are not enabled on Google Cloud Platform. 

> Function calling is not enabled for {model}

Gemma 3 does not have native [function call](https://ai.google.dev/gemma/docs/capabilities/function-calling#function-calling-setup) support. Gemma 3 expects a standard two-role conversation between a user and an assistant, with no "tool" role in between. You can't have an effective agent without prompt engineering where the user translates the function result to a natural statement. There are community models fine tuned for better tool support. 

Google released [FunctionGemma](https://blog.google/innovation-and-ai/technology/developers-tools/functiongemma/) to address this issue. But it refuses to process any requests that suggest business:

> I cannot assist with drafting or estimating project proposals. My current capabilities are focused on managing job postings and tool data analysis. I cannot generate strategic business proposals or generate detailed financial projections for projects requiring project management or cost estimation.

Gemma 3 supports structured output on llama.cpp but not on Microsoft Foundry for the same reason as Phi 4. 

|Model|llama.cpp|Google Cloud Platform |
|-|:-:|:-:|
|Gemma 3 270M It|âŒ<br/>âŒ||
|Gemma 3 1B It|âœ…<br/>âŒ|âŒ<br/>âŒ|
|Gemma 3 4B It|âœ…<br/>âŒ|âŒ<br/>âŒ|
|Gemma 3 12B It||âŒ<br/>âŒ|
|Gemma 3 27B It||âŒ<br/>âŒ|
|Gemma 3N E4B It||âŒ<br/>âŒ|
|Gemma 3N E2B It||âŒ<br/>âŒ|
|FunctionGemma|âŒ<br/>âŒ|

### Qwen 

**Qwen** is an open weight model developed by Alibaba. Their flagship `Qwen3-235B-A22B` is hosted by Alibaba Cloud and multiple providers including [Fireworks AI](https://fireworks.ai) and [DeepInfra](https://deepinfra.com).  Qwen 2.5 and 3 both have native function call support.

The smaller models may lack thinking capabilty to plan a sequence of tool calls if the prompt is too vague. That said, Qwen 3 is the only model in my testing that supports both structured outputs and tool calling on llama.cpp. ğŸ†

|Model|llama.cpp|Azure OpenAI|DeepInfra |
|-|:-:|:-:|:-:|
|Qwen 3 0.6B|âœ…<br/>âš ï¸||
|Qwen 3 1.7B|âœ…<br/>âš ï¸||
|Qwen 3 4B Thinking 2507|âœ…<br/>âœ…||
|Qwen 3 4B Instruct 2507|âœ…<br/>âœ…||
|Qwen 3 14B |||âŒ<br/>âŒ
|Qwen 3 32B ||âŒ<br/>âŒ|âœ…<br/>âŒ
|Qwen 3 235B Thinking|||âœ…<br/>âŒ

### OpenAI GPT

For function calling you would want to use a **reasoning** (thinking, chain of thought) model. The first reasoning model from OpenAI is **4o** which was released between 4 and 4.1. 4.1 is the last non-reasoning model. After 4.1 came o1, o3, and o4 which are all reasoning models. GPT 5 series are all reasoninig models. As of today, 3.5, 4o, 4.1, o1, o3, o4 are legacy models.

|Model||Remarks
|-|:-:|-|
|3.5|âŒ<br/>âŒ|This is not a chat model and thus not supported in the v1/chat/completions endpoint. 
|3.5-turbo|âŒ<br/>âŒ|This is not a chat model and thus not supported in the v1/chat/completions endpoint. 
|o1|âŒ<br/>âŒ|This model is only supported in v1/responses and not in v1/chat/completions.
|o3|âŒ<br/>âŒ|This is not a chat model and thus not supported in the v1/chat/completions endpoint. 
|o4|âŒ<br/>âŒ|Unsupported value: 'temperature' does not support 0 with this model. Only the default (1) value is supported.
|4|âŒ<br/>âŒ|
|4o|âœ…<br/>âœ…|
|4-turbo|âœ…<br/>âœ…|
|4.1|âœ…<br/>âœ…|
|5|âŒ<br/>âŒ|This model is only supported in v1/responses and not in v1/chat/completions.
|5.1|âœ…<br/>âœ…|
|5.2|âœ…<br/>âœ…|

> Use 4o, 4-turbo, 4.1, 5.1, or 5.2.

### Gemini

|Model||
|-|:-:|
|2.0|âŒ<br/>âŒ
|2.5|âœ…<br/>âœ…|
|3|âŒ<br/>âŒ|

> Use 2.5. Gemini 3 (preview) on OpenAI compatibility seems to have a regression.

### Claude 

|Model||Remarks
|-|:-:|:-|
|Haiku 4.5|âœ…<br/>âœ…|
|Opus 4.5|âœ…<br/>âœ…|
|Sonnet 4.5|âœ…<br/>âœ…|Request might exceed the rate limit of 10,000 input tokens per minute.

> Use Haiku or Opus if you have a low quota.
